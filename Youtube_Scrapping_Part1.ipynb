{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import xlsxwriter\n",
    "from datetime import datetime\n",
    "\n",
    "api_key = '' #Please insert your key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the daily data extraction limitations of the YouTube API, multiple extraction runs are required. Subsequent data pulls will be merged with previously accumulated data, up to the cut-off time of the prior runs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the specific video IDs from the videos appearing from the searches using the keywords below\n",
    "keywords = ['Abarth EV', 'Abarth Combustion', 'Fiat combustion engine', 'Fiat 500e', 'Tesla EV', \n",
    "            'Mini Cooper combustion cars', 'Mini Cooper EV', 'Peugeot Combustion','Peugeot EV', \n",
    "            'Volkswagen combustion engine', 'Volkswagen EV']\n",
    "video_ids = []\n",
    "\n",
    "\n",
    "# This date changes to when the previous run date was cut off. To scrap the latest videos in the next run\n",
    "after_date = '2023-06-02T00:00:00Z'\n",
    "\n",
    "for keyword in keywords:\n",
    "    search_response = youtube.search().list(\n",
    "        q=keyword,\n",
    "        part='id,snippet',\n",
    "        maxResults=10,\n",
    "        type='video',\n",
    "        publishedAfter=after_date\n",
    "    ).execute()\n",
    "    \n",
    "    # Extract video IDs\n",
    "    for item in search_response['items']:\n",
    "        video_id = item['id']['videoId']\n",
    "        video_ids.append(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of video_ids\n",
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty variables for data collection\n",
    "df1_data = []\n",
    "df2_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF1 is the general information about the videos to extract the comments from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section for df1\n",
    "\n",
    "#Filling it into the \"df1_data\"\n",
    "for i in range(0, len(video_ids), 10):\n",
    "    ids_chunk = video_ids[i:i+10]\n",
    "    video_response = youtube.videos().list(\n",
    "        id=','.join(ids_chunk),\n",
    "        part='snippet,statistics'\n",
    "    ).execute()\n",
    "    \n",
    "    for video in video_response['items']:\n",
    "        video_id = video['id']\n",
    "\n",
    "        # Extract data for DataFrame 1\n",
    "        post_id = video_id\n",
    "        post_title = video['snippet']['title']\n",
    "        author = video['snippet']['channelTitle']\n",
    "        date = video['snippet']['publishedAt']\n",
    "        post_content = video['snippet']['description']\n",
    "        comment_number = int(video['statistics'].get('commentCount', '0'))\n",
    "        net_like = int(video['statistics'].get('likeCount', '0')) - int(video['statistics'].get('dislikeCount', '0'))\n",
    "        views = video['statistics']['viewCount']\n",
    "        df1_data.append([post_id, post_title, author, date, post_content, comment_number, net_like, views])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF2 is the comment and specific details of the comments from the videos in DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping replies for comment UgwEm1IH1L0CGh6nOPB4AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=UgwEm1IH1L0CGh6nOPB4AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Skipping replies for comment UgyEwtJArc4U9yhJdJ14AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=UgyEwtJArc4U9yhJdJ14AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Skipping replies for comment UgwEbBNqbxj5RwrsVoR4AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=UgwEbBNqbxj5RwrsVoR4AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Skipping replies for comment UgwXXenjSt9UUpEWvQd4AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=UgwXXenjSt9UUpEWvQd4AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Skipping replies for comment Ugz0Ts3ZiAzBpgMzqc54AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=Ugz0Ts3ZiAzBpgMzqc54AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Skipping replies for comment Ugw3GgretUmkwOXtk5V4AaABAg due to error: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/comments?part=id%2Csnippet&parentId=Ugw3GgretUmkwOXtk5V4AaABAg&maxResults=100&textFormat=plainText&key=AIzaSyDvumDw7vmA8JNrDVSqAISSmSPQ0ihESzc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n"
     ]
    }
   ],
   "source": [
    "# Fetch comments from videos\n",
    "#Section for df2\n",
    "next_page_token = None\n",
    "while True:\n",
    "    try:\n",
    "        comments_response = youtube.commentThreads().list(\n",
    "            videoId=video_id,\n",
    "            part='id,snippet',\n",
    "            maxResults=50,\n",
    "            textFormat='plainText',\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        # Extract data for DataFrame 2\n",
    "        for item in comments_response['items']:\n",
    "            unique_id = item['id']\n",
    "            post_id = video_id\n",
    "            author = item['snippet']['topLevelComment']['snippet'].get('authorDisplayName', 'Unknown')\n",
    "            date = item['snippet']['topLevelComment']['snippet'].get('publishedAt', 'Unknown')\n",
    "            comment_content = item['snippet']['topLevelComment']['snippet'].get('textDisplay', '')\n",
    "            reply_id = item['snippet'].get('totalReplyCount', '0')\n",
    "\n",
    "            df2_data.append([unique_id, post_id, author, date, comment_content, reply_id])\n",
    "\n",
    "            # Fetch replies for the current top level comment\n",
    "            if reply_id > 0:\n",
    "                reply_next_page_token = None\n",
    "                while True:\n",
    "                    try:\n",
    "                        reply_response = youtube.comments().list(\n",
    "                            part='id,snippet',\n",
    "                            parentId=unique_id,\n",
    "                            maxResults=100,\n",
    "                            textFormat='plainText',\n",
    "                            pageToken=reply_next_page_token\n",
    "                        ).execute()\n",
    "\n",
    "                        #Extract data for replies\n",
    "                        for reply_item in reply_response['items']:\n",
    "                            reply_unique_id = reply_item['id']\n",
    "                            reply_author = reply_item['snippet'].get('authorDisplay', 'Unknown')\n",
    "                            reply_date = reply_item['snippet'].get('publishedAt', 'Unknown')\n",
    "                            reply_comment_content = reply_item['snippet'].get('textDisplay', '')\n",
    "                            parent_id = unique_id #mapping to the parent comment ID\n",
    "\n",
    "                            df2_data.append([reply_unique_id, post_id, reply_author, reply_date, reply_comment_content,\n",
    "                            0, parent_id])\n",
    "\n",
    "                            reply_next_page_token = reply_response.get('nextPageToken')\n",
    "                            if not reply_next_page_token:\n",
    "                                break\n",
    "                    except HttpError as e:\n",
    "                        print(f\"Skipping replies for comment {unique_id} due to error: {e}\")\n",
    "                        break\n",
    "\n",
    "        # Check if there are more comments to fetch\n",
    "        next_page_token = comments_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Skipping video {video_id} due to error: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes\n",
    "df1_columns = ['Post_id', 'Post_Title', 'Author', 'Date', 'Post_Content', 'Comment_Number', 'Net_Like', 'Views']\n",
    "df1 = pd.DataFrame(df1_data, columns=df1_columns)\n",
    "\n",
    "df2_columns = ['Unique_id', 'Post_id', 'Author', 'Date', 'Comment_Content', 'Reply_Count', 'Parent_id']\n",
    "df2 = pd.DataFrame(df2_data, columns=df2_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8906"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_id</th>\n",
       "      <th>Post_Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Post_Content</th>\n",
       "      <th>Comment_Number</th>\n",
       "      <th>Net_Like</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6fDMXjYOsGM</td>\n",
       "      <td>2023 Abarth 500E Turismo First Drive Review: A...</td>\n",
       "      <td>Stef ABtv</td>\n",
       "      <td>2023-07-19T05:30:11Z</td>\n",
       "      <td>Here is the NEW Abarth 500e Turismo Electric F...</td>\n",
       "      <td>134</td>\n",
       "      <td>381</td>\n",
       "      <td>14241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iak-pj6sdFA</td>\n",
       "      <td>IS THIS A REAL ABARTH?! Driving the 500e Elect...</td>\n",
       "      <td>Auto Social UK</td>\n",
       "      <td>2023-07-19T04:45:01Z</td>\n",
       "      <td>Mt expectations were actually not all that hig...</td>\n",
       "      <td>106</td>\n",
       "      <td>426</td>\n",
       "      <td>7123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Ya6yWohTW4</td>\n",
       "      <td>Abarth 500 Electric! - I Might Buy One! Eventu...</td>\n",
       "      <td>Electric Vehicle Man</td>\n",
       "      <td>2023-07-21T15:30:00Z</td>\n",
       "      <td>We test the new (and arguably 1st) electric ho...</td>\n",
       "      <td>202</td>\n",
       "      <td>987</td>\n",
       "      <td>22741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OqQo6VLmnTc</td>\n",
       "      <td>Abarth’s NOISY EV Doesn't Care About Speed!</td>\n",
       "      <td>Fully Charged Show</td>\n",
       "      <td>2023-07-20T14:00:24Z</td>\n",
       "      <td>Jack and Bobby go for a test drive in the firs...</td>\n",
       "      <td>491</td>\n",
       "      <td>4972</td>\n",
       "      <td>142757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigX6_seQr4</td>\n",
       "      <td>New Abarth 500e: Mission Possible. The Mission...</td>\n",
       "      <td>Abarth</td>\n",
       "      <td>2023-06-30T14:13:54Z</td>\n",
       "      <td>It’s no easy feat when you're presented with a...</td>\n",
       "      <td>21</td>\n",
       "      <td>177</td>\n",
       "      <td>565333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Post_id                                         Post_Title  \\\n",
       "0  6fDMXjYOsGM  2023 Abarth 500E Turismo First Drive Review: A...   \n",
       "1  iak-pj6sdFA  IS THIS A REAL ABARTH?! Driving the 500e Elect...   \n",
       "2  -Ya6yWohTW4  Abarth 500 Electric! - I Might Buy One! Eventu...   \n",
       "3  OqQo6VLmnTc        Abarth’s NOISY EV Doesn't Care About Speed!   \n",
       "4  bigX6_seQr4  New Abarth 500e: Mission Possible. The Mission...   \n",
       "\n",
       "                 Author                  Date  \\\n",
       "0             Stef ABtv  2023-07-19T05:30:11Z   \n",
       "1        Auto Social UK  2023-07-19T04:45:01Z   \n",
       "2  Electric Vehicle Man  2023-07-21T15:30:00Z   \n",
       "3    Fully Charged Show  2023-07-20T14:00:24Z   \n",
       "4                Abarth  2023-06-30T14:13:54Z   \n",
       "\n",
       "                                        Post_Content  Comment_Number  \\\n",
       "0  Here is the NEW Abarth 500e Turismo Electric F...             134   \n",
       "1  Mt expectations were actually not all that hig...             106   \n",
       "2  We test the new (and arguably 1st) electric ho...             202   \n",
       "3  Jack and Bobby go for a test drive in the firs...             491   \n",
       "4  It’s no easy feat when you're presented with a...              21   \n",
       "\n",
       "   Net_Like   Views  \n",
       "0       381   14241  \n",
       "1       426    7123  \n",
       "2       987   22741  \n",
       "3      4972  142757  \n",
       "4       177  565333  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_id</th>\n",
       "      <th>Post_id</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment_Content</th>\n",
       "      <th>Reply_Count</th>\n",
       "      <th>Parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgxfsDMcNEp9gnixiVV4AaABAg</td>\n",
       "      <td>XyfhwrRi92M</td>\n",
       "      <td>J HEFF</td>\n",
       "      <td>2023-08-05T11:53:36Z</td>\n",
       "      <td>Grown...... You mean forced upon the proletari...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgzGp9nBrOzVltT4CJx4AaABAg</td>\n",
       "      <td>XyfhwrRi92M</td>\n",
       "      <td>Finn Carl Bomholt Sørensen</td>\n",
       "      <td>2023-08-05T07:14:40Z</td>\n",
       "      <td>Toyota has also stopped development of EVs</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgzKEZCC4CGgn8ia5TB4AaABAg</td>\n",
       "      <td>XyfhwrRi92M</td>\n",
       "      <td>bassmanjr100</td>\n",
       "      <td>2023-08-03T23:30:06Z</td>\n",
       "      <td>No idea if true but if it is I'm glad someone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugxegzu6Y4R1La04I_94AaABAg</td>\n",
       "      <td>XyfhwrRi92M</td>\n",
       "      <td>Steven</td>\n",
       "      <td>2023-08-02T22:23:18Z</td>\n",
       "      <td>Volks waygin?</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugz6JF-dwis_UVK8Bht4AaABAg</td>\n",
       "      <td>XyfhwrRi92M</td>\n",
       "      <td>Steven</td>\n",
       "      <td>2023-08-02T22:18:16Z</td>\n",
       "      <td>It would seem that it will take the manufactur...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unique_id      Post_id                      Author  \\\n",
       "0  UgxfsDMcNEp9gnixiVV4AaABAg  XyfhwrRi92M                      J HEFF   \n",
       "1  UgzGp9nBrOzVltT4CJx4AaABAg  XyfhwrRi92M  Finn Carl Bomholt Sørensen   \n",
       "2  UgzKEZCC4CGgn8ia5TB4AaABAg  XyfhwrRi92M                bassmanjr100   \n",
       "3  Ugxegzu6Y4R1La04I_94AaABAg  XyfhwrRi92M                      Steven   \n",
       "4  Ugz6JF-dwis_UVK8Bht4AaABAg  XyfhwrRi92M                      Steven   \n",
       "\n",
       "                   Date                                    Comment_Content  \\\n",
       "0  2023-08-05T11:53:36Z  Grown...... You mean forced upon the proletari...   \n",
       "1  2023-08-05T07:14:40Z         Toyota has also stopped development of EVs   \n",
       "2  2023-08-03T23:30:06Z  No idea if true but if it is I'm glad someone ...   \n",
       "3  2023-08-02T22:23:18Z                                      Volks waygin?   \n",
       "4  2023-08-02T22:18:16Z  It would seem that it will take the manufactur...   \n",
       "\n",
       "   Reply_Count Parent_id  \n",
       "0            0      None  \n",
       "1            0      None  \n",
       "2            0      None  \n",
       "3            0      None  \n",
       "4            0      None  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining with old and removing duplicates before exporting it to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_old = pd.read_excel('youtube_data_latest.xlsx', sheet_name=0)\n",
    "df2_old = pd.read_excel('youtube_data_latest.xlsx', sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_combined = pd.concat([df1_old, df1], ignore_index=True)\n",
    "df2_combined = pd.concat([df2_old, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_combined.drop_duplicates(inplace=True)\n",
    "df2_combined.drop_duplicates(subset=['Comment_Content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148707"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/slyrm7113271w310_rtxsbnw0000gn/T/ipykernel_28566/3321089266.py:9: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "writer = pd.ExcelWriter('youtube_data_latest.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each combined dataframe to a different worksheet.\n",
    "df1_combined.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "df2_combined.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f3c74e9254d476e4b254d69434b9f89897df3269b6a5d96ebf52627a70691ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
